{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba Tecnica LT Challenge\n",
    "\n",
    "En este notebook, se enfocará en los dos enfoques diferentes para cada una de las funciones, tiempo de ejecución y optimización de memoria. Tomaremos q2 como base de nuestra evaluación debido a que la linea logica es similar y se repite en todas las funciones. Asi como las primeras versiones del codigo la tomaremos como benchmark para evaluar resultados. Ademas, anadiremos el siguiente banco de ideas donde disponeremos de conceptos o tecnologias que podamos incluir o evaluar hasta la entrega de este proyecto:\n",
    "\n",
    "| Foco            | Concepto                                                         | Estado                           |\n",
    "|-----------------|------------------------------------------------------------------|----------------------------------|\n",
    "| tiempo & memoria| **Pandas Basico**: Libreria estandar para procesamiento de datos | Incluida 1a version              |\n",
    "| tiempo & memoria| **Chunks**: Es necesario particionar los datos                   | Incluida 1a version              |\n",
    "| tiempo          | **Multiprocessing**: Procesamiento paralelo                      | Pendiente                        |\n",
    "| tiempo          | **Polars**: Libreria escrita en Rust                             | Pendiende *Valdria la pena evaluarlo* |\n",
    "| memoria         | **Generators**: Lazy evaluation e iterators sirven para optimizacion de memoria | Pendiente                        |\n",
    "| memoria         | **Pandas Avanzado**: Aplicar tecnicas como sparse                | pendiente                        |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "## Análisis top 10 de los emojis más usados en los Tweets\n",
    "\n",
    "### Preprocesamiento e Importación de Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import emoji\n",
    "import time\n",
    "import os\n",
    "from typing import List, Tuple\n",
    "from memory_profiler import memory_usage\n",
    "from memory_profiler import profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from q2_memory import q2_memory\n",
    "from stats import get_file_path\n",
    "\n",
    "# Usaremos los datos proveídos en el challenge pero desde google cloud storage\n",
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = \"../sa.json\"\n",
    "bucket_name = 'test_jobs144'\n",
    "file_name = 'lt/farmers-protest-tweets-2021-2-4.json'\n",
    "temp_file_path = get_file_path(bucket_name, file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Codigo Benchmark\n",
    "Este código fue desarrollado de forma básica a los dos días de iniciar el challenge, con el fin de tener una base para trabajar. Esta función es igual para q2_memory y q2_time por practicidad; a lo largo del proyecto, esto cambiará.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n",
    "\n",
    "@profile\n",
    "def q2(file_path: str) -> List[Tuple[str, int]]:\n",
    "    \"\"\"\n",
    "    Analyze Twitter data to count emojis for the top 10 emojis.\n",
    "    \n",
    "    Args:\n",
    "    file_path (str): Path to file containing Twitter data.\n",
    "    \n",
    "    Returns:\n",
    "    List[Tuple[str, int]]: Top 10 days with most emojis and their emoji counts\n",
    "    \"\"\"\n",
    "    \n",
    "    chunks = pd.read_json(file_path, lines=True, chunksize=10000) # Particionamos en chunks de 10000 lineas cada uno\n",
    "    \n",
    "    dfs = []\n",
    "    \n",
    "    for chunk in chunks: # Por cada chuck extraemos un dataframe con los emojis y sus contadores\n",
    "        emojis_list = chunk['content'].apply(extract_emojis).explode() # Extraemos series.pandas continiendo lista de emojis para luego aplanarlos en una serie de strings \n",
    "        emojis_count = emojis_list.to_frame(name='emoji').groupby('emoji').size().reset_index(name='count') # Realizamos conteo por emoji\n",
    "        dfs.append(emojis_count)\n",
    "    \n",
    "    all_counts = pd.concat(dfs) # Concatenamos dataframes derivados de los chunks\n",
    "    final_counts = all_counts.groupby('emoji')['count'].sum().reset_index() # Realizamos la correspondiente agregacion de acuerdo al emoji\n",
    "    top_10_days = final_counts.sort_values('count', ascending=False).head(10) # Seleccionamos el top 10\n",
    "    \n",
    "    return [(row['emoji'], int(row['count'])) for _, row in top_10_days.iterrows()]\n",
    "\n",
    "def extract_emojis(text: str) -> list: # Extrae todos los emojis que pueda tener un tweet\n",
    "    \"\"\"Get emojis in a given text.\"\"\"\n",
    "    return [x for x in text if x in emoji.EMOJI_DATA]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraemos metricas de uso memoria y tiempo de procesamiento Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find file /tmp/ipykernel_1599/4263676936.py\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Filename: /usr/local/lib/python3.11/site-packages/memory_profiler.py\n",
       "\n",
       "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
       "=============================================================\n",
       "  1185    279.9 MiB    279.9 MiB           1               @wraps(wrapped=func)\n",
       "  1186                                                     def wrapper(*args, **kwargs):\n",
       "  1187    279.9 MiB      0.0 MiB           1                   prof = get_prof()\n",
       "  1188    338.8 MiB     58.9 MiB           1                   val = prof(func)(*args, **kwargs)\n",
       "  1189    338.8 MiB      0.0 MiB           1                   show_results_bound(prof)\n",
       "  1190    338.8 MiB      0.0 MiB           1                   return val"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%mprun -f q2 q2(temp_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find file /tmp/ipykernel_1599/4263676936.py\n"
     ]
    }
   ],
   "source": [
    "mem_benchmark_usage = memory_usage((q2, (temp_file_path,), {}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average memory usage: 427.6349664135514 MiB\n",
      "Maximum memory usage: 471.4140625 MiB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Average memory usage: {sum(mem_benchmark_usage) / len(mem_benchmark_usage)} MiB\")\n",
    "print(f\"Maximum memory usage: {max(mem_benchmark_usage)} MiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.8084704875946045\n",
      "1727450197.4192607\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "exam = q1_memory(file_path)\n",
    "end_time = time.time()\n",
    "total_time = end_time - start_time\n",
    "print(total_time)\n",
    "print(end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('🤫', 2), ('🤔', 2), ('🌾', 1), ('🚜', 1), ('💪', 1)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exam = q2_memory(test_file_path)\n",
    "exam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mandeeppunia1', 2),\n",
       " ('DelhiPolice', 1),\n",
       " ('Kisanektamorcha', 1),\n",
       " ('ReallySwara', 1),\n",
       " ('narendramodi', 1),\n",
       " ('rohini_sgh', 1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exam = q3_memory(test_file_path)\n",
    "exam"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
